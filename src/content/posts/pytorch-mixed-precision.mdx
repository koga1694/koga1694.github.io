---
title: 'PyTorch Mixed Precision Training으로 학습 속도 2배 향상시키기'
description: 'AMP(Automatic Mixed Precision)를 활용해 메모리 사용량을 줄이고 학습 속도를 높이는 방법을 알아봅니다.'
pubDate: 2024-01-20
category: 'tutorial'
tags: ['pytorch', 'deep-learning', 'optimization', 'gpu']
---

## Mixed Precision이란?

Mixed Precision Training은 FP32와 FP16을 혼합하여 사용하는 학습 방법입니다.

| Precision | Bits | Range | 용도 |
|-----------|------|-------|------|
| FP32 | 32 | $\pm 3.4 \times 10^{38}$ | 기본 연산 |
| FP16 | 16 | $\pm 65504$ | 행렬 연산 |
| BF16 | 16 | $\pm 3.4 \times 10^{38}$ | A100+ GPU |

## 기본 사용법

PyTorch 2.0+에서는 `torch.amp`를 사용합니다:

```python
import torch
from torch.amp import autocast, GradScaler

model = MyModel().cuda()
optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)
scaler = GradScaler()

for batch in dataloader:
    optimizer.zero_grad()

    with autocast(device_type='cuda', dtype=torch.float16):
        outputs = model(batch['input'].cuda())
        loss = criterion(outputs, batch['target'].cuda())

    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

## PyTorch Lightning에서 사용

```python
import pytorch_lightning as pl

trainer = pl.Trainer(
    accelerator='gpu',
    precision='16-mixed',  # or 'bf16-mixed' for A100
    max_epochs=100,
)
```

## 성능 비교

실험 환경: RTX 4090, ResNet-50, ImageNet

$$
\text{Speedup} = \frac{T_{FP32}}{T_{FP16}} \approx 2.1\times
$$

| 설정 | 배치 크기 | 학습 시간/epoch | 메모리 |
|------|----------|-----------------|--------|
| FP32 | 64 | 45분 | 18GB |
| FP16 | 128 | 22분 | 12GB |
| BF16 | 128 | 21분 | 12GB |

## 주의사항

1. **Loss Scaling**: FP16의 좁은 범위로 인해 gradient underflow 발생 가능
2. **특정 연산**: BatchNorm, Softmax 등은 FP32 유지 권장
3. **수렴 문제**: 학습률 조정이 필요할 수 있음

## Loss Scaling 이해하기

Gradient의 크기가 작을 때 FP16 범위를 벗어나는 것을 방지:

$$
\text{scaled\_loss} = \text{loss} \times s
$$

$$
\text{gradient} = \frac{\nabla \text{scaled\_loss}}{s}
$$

여기서 $s$는 동적으로 조정되는 scale factor입니다.

## 결론

Mixed Precision은 거의 모든 딥러닝 학습에서 활성화해야 하는 필수 최적화 기법입니다.
특히 메모리가 부족한 환경에서 더 큰 배치 크기를 사용할 수 있어 학습 안정성도 향상됩니다.
