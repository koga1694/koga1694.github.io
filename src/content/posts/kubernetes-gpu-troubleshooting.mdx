---
title: 'Kubernetes에서 GPU Pod이 Pending 상태로 멈출 때 해결 방법'
description: 'EKS 클러스터에서 GPU 노드 스케일링과 관련된 트러블슈팅 경험을 공유합니다. NVIDIA device plugin과 node selector 설정의 중요성에 대해 다룹니다.'
pubDate: 2024-01-15
category: 'troubleshooting'
tags: ['kubernetes', 'gpu', 'eks', 'nvidia', 'mlops']
---

## 문제 상황

ML 모델 추론 서버를 EKS에 배포하던 중, GPU를 요청하는 Pod이 계속 `Pending` 상태로 남아있는 문제가 발생했습니다.

```bash
kubectl get pods -n inference
NAME                              READY   STATUS    RESTARTS   AGE
model-server-7b4d5c6f8-x9k2l      0/1     Pending   0          15m
```

## 원인 분석

`kubectl describe pod`로 확인해보니 다음과 같은 이벤트가 있었습니다:

```bash
Events:
  Type     Reason            Age   From               Message
  ----     ------            ----  ----               -------
  Warning  FailedScheduling  12m   default-scheduler  0/5 nodes are available:
           5 Insufficient nvidia.com/gpu.
```

문제는 크게 세 가지였습니다:

1. **NVIDIA Device Plugin 미설치**: GPU 노드에 device plugin DaemonSet이 없었음
2. **Node Selector 누락**: GPU 노드를 정확히 타겟팅하지 않음
3. **Cluster Autoscaler 설정 오류**: GPU 노드 그룹이 스케일업 대상에서 빠져있었음

## 해결 방법

### 1. NVIDIA Device Plugin 설치

```bash
kubectl apply -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.14.1/nvidia-device-plugin.yml
```

### 2. Pod Spec 수정

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: model-server
spec:
  template:
    spec:
      nodeSelector:
        node.kubernetes.io/instance-type: g4dn.xlarge
      tolerations:
        - key: "nvidia.com/gpu"
          operator: "Exists"
          effect: "NoSchedule"
      containers:
        - name: inference
          resources:
            limits:
              nvidia.com/gpu: 1
```

### 3. Cluster Autoscaler 설정 확인

Cluster Autoscaler의 `--nodes` 플래그에 GPU 노드 그룹이 포함되어 있는지 확인합니다:

```yaml
- --nodes=0:10:eks-gpu-node-group
```

## GPU 메모리 계산

VRAM 용량에 따른 배치 사이즈 계산 공식:

$$
\text{Max Batch Size} = \frac{\text{Available VRAM} - \text{Model Size}}{\text{Per-Sample Memory}}
$$

예를 들어, 16GB GPU에서 4GB 모델을 로드하고 샘플당 500MB가 필요하다면:

$$
\text{Max Batch Size} = \frac{16\text{GB} - 4\text{GB}}{0.5\text{GB}} = 24
$$

## 결론

GPU 관련 이슈는 대부분 Device Plugin 설치 여부와 노드 셀렉터 설정에서 발생합니다.
EKS에서 GPU 워크로드를 운영할 때는 반드시 이 두 가지를 먼저 확인하세요.

## 참고 자료

- [NVIDIA Device Plugin for Kubernetes](https://github.com/NVIDIA/k8s-device-plugin)
- [AWS EKS GPU Support](https://docs.aws.amazon.com/eks/latest/userguide/gpu-ami.html)
