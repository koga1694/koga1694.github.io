---
title: "Kubernetes Pod OOMKilled 에러 디버깅"
description: "메모리 부족으로 인한 Pod 종료 문제 해결 가이드"
publishDate: 2026-01-15
tags: ["kubernetes", "troubleshooting", "mlops"]
category: "troubleshooting"
featured: true
draft: false
---

## 문제 상황

ML 트레이닝 작업 중 Pod가 OOMKilled로 종료되는 경우를 경험했습니다. 이 문제는 특히 대규모 모델을 학습할 때 자주 발생합니다.

```bash
kubectl get pods
NAME                          READY   STATUS      RESTARTS   AGE
ml-training-job-abc123        0/1     OOMKilled   0          5m
```

## 원인 분석

OOMKilled는 Out Of Memory Killed의 약자로, 컨테이너가 할당된 메모리 한도를 초과했을 때 발생합니다.

주요 원인:
- 메모리 limits 설정이 너무 낮음
- 배치 크기가 너무 큼
- 메모리 누수 (memory leak)

## 해결 방법

### 1. 메모리 Limits 증가

```yaml
# pod-config.yaml
apiVersion: v1
kind: Pod
metadata:
  name: ml-training-job
spec:
  containers:
  - name: trainer
    image: pytorch-trainer:latest
    resources:
      requests:
        memory: "8Gi"
        cpu: "4"
      limits:
        memory: "16Gi"
        cpu: "8"
```

### 2. 배치 크기 조정

메모리 사용량을 추정하는 공식:

$$
\text{Memory (GB)} = \frac{\text{Batch Size} \times \text{Model Params} \times 4}{1024^3}
$$

예를 들어, 1억 파라미터 모델에 배치 크기 32를 사용하면:

$$
\frac{32 \times 100,000,000 \times 4}{1024^3} \approx 11.92 \text{ GB}
$$

### 3. 메모리 모니터링

```python
import psutil
import torch

def log_memory_usage():
    process = psutil.Process()
    mem_info = process.memory_info()
    print(f"RAM: {mem_info.rss / 1024 ** 3:.2f} GB")
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB")

# 학습 루프 내에서 주기적으로 호출
log_memory_usage()
```

## 결론

적절한 메모리 설정과 모니터링을 통해 OOMKilled 문제를 예방할 수 있습니다. 프로덕션 환경에서는 항상 여유 메모리를 확보하는 것이 중요합니다.
